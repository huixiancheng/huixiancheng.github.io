<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <meta name="viewport" content="width=800">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <style type="text/css">
        /* Color scheme stolen from Sergey Karayev */
        a {
            color: #1772d0;
            text-decoration: none;
        }

        a:focus, a:hover {
            color: #f09228;
            text-decoration: none;
        }

        body, td, th, tr, p, a {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 14px
        }

        .hp-photo {
            width: 240px;
            height: 240px;
            border-radius: 240px;
            -webkit-border-radius: 240px;
            -moz-border-radius: 240px;
        }

        strong {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 14px;
        }

        heading {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 24px;
        }

        papertitle {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 15px;
            font-weight: 700
        }

        name {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 32px;
        }

        .one {
            width: 160px;
            height: 160px;
            position: relative;
        }

        .two {
            width: 160px;
            height: 160px;
            position: absolute;
            transition: opacity .2s ease-in-out;
            -moz-transition: opacity .2s ease-in-out;
            -webkit-transition: opacity .2s ease-in-out;
        }

        .fade {
            transition: opacity .2s ease-in-out;
            -moz-transition: opacity .2s ease-in-out;
            -webkit-transition: opacity .2s ease-in-out;
        }

        span.highlight {
            background-color: #ffffd0;
        }
    </style>

    <title>Huixian Cheng (程辉先)</title>
    <!--<link rel="stylesheet" type="text/css" href="/imgs/css" >-->
    <link rel="icon" type="image/jpg" href="images/yui.jpg">
</head>

<meta name="viewport" content="width=device-width">
<style>
html .sakana-box{
  position: fixed;
  right: 0;
  bottom: 0;

  transform-origin: 100% 100%; /* 从右下开始变换 */
}
</style>

<div class="sakana-box"></div>

<script src="https://cdn.jsdelivr.net/npm/sakana@1.0.8"></script>
<script>
// 取消静音
Sakana.setMute(false);

// 启动
Sakana.init({
  el:         '.sakana-box',     // 启动元素 node 或 选择器
  scale:      .5,                // 缩放倍数
  canSwitchCharacter: true,      // 允许换角色
});
</script>

<body>
<table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tbody>
    <tr>
        <td>


            <!--SECTION 1 -->
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                <tr>
                    <td width="68%" valign="middle">
                        <p align="center">
                            <name>Huixian Cheng</name>
                        </p>
                        <p align="justify">I am a final-year master's student major in CS in the College of Computer and Information
                            Science at <a href="http://www.swu.edu.cn/">Southwest University</a>, Chongqing, China.
                            Advised by both <a href="http://cis.swu.edu.cn/info/1013/1150.htm">Prof. Guoqiang Xiao</a>
                            and <a href="https://scholar.google.com/citations?user=4FaCTFgAAAAJ&hl=en&oi=ao">Prof.
                            Xianfeng Han.</a> I obtained my bachelor's degree in Naval Architecture and Ocean Engineering
                            from <a href="https://www.whut.edu.cn/">Wuhan University of Technology</a>, in 2019.

                            <!--                    </br></br>-->
                            <!--                    In this summer (July - Oct 2019), I was a research intern at the Augumented Reality team of <a href="http://www.a9.com/">Amazon</a> (Palo Alto, CA).-->
                            <!--                    In my M.Phil study, I interned at <a href="https://www.astri.org/">Hong Kong Applied Science and Technology Research Institute</a>.-->
                            <!--                    In my undergraduate study, I was an exchange student at <a href="http://www.upv.es/">Universitat Politècnica de València</a> (Valencia, Spain).-->

                            </br>
                        </p>
                        <p align="center">
                            <a href="mailto:chenghuixian@email.swu.edu.cn">Email</a> /
                            <a href="https://github.com/huixiancheng"> Github </a> /
                            <a href="https://scholar.google.com/citations?user=L39a9d8AAAAJ&hl=en">Google Scholar</a>
                        </p>
                    </td>
                    <td align="right"><img class="hp-photo" src="./images/chx.jpg" style="width:100%;max-width:100%"></td>
                </tr>
                </tbody>
            </table>

            <!--SECTION 2 -->
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                <tr>
                    <td>
                        <heading>Research</heading>
                        <p style="width:65%;">
                            Currently, I am focusing on the following topics.
                        </p>

                        <ol>
                            <li><p>
                                <strong>3D Scene Understanding</strong>: <br>
                                <p>
                                    Efficient semantic segmentation in autonomous driving scenarios.
                                </p></li>
                            <li><p>
                                <strong>Multi-Sensor Fusion</strong>: <br>
                                <p>
                                    I am also very interested in sensor fusion, especially between vision cameras and
                                    LiDAR.
                                </p></li>
                        </ol>
                    </td>
                </tr>
                </tbody>
            </table>

            <!--SECTION 3 -->
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                <tr>
                    <td>
                        <heading>News</heading>
                        <p><strong>[2022.07.01 - Now]</strong> <font color="red">Seeking jobs in autonomous driving perception algorithm</font>.
                        <p><strong>[2022.09.30]</strong> PCB-RandNet has been submitted to RA-L.
                        <p><strong>[2022.09.27]</strong> PCB-RandNet has been rejected by AAAI 2023.
                        <p><strong>[2022.03.16]</strong> CENet has been accepted by ICME 2022.
                        <p><strong>[2021.12.02]</strong> TransRVNet has been submitted to IEEE T-ITS.

                    </td>
                </tr>
                </tbody>
            </table>

            <!--SECTION 4 -->
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                <tr>
                    <td>
                        <heading>Publications / Preprints</heading>
                    </td>
                </tr>
                </tbody>
            </table>

            <!--SECTION-->
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                <tr>
                    <td width="20%"><img src="images/ppl.png" alt="PontTuset" width="180"
                                         style="border-style: none"></td>
                    <td width="80%" valign="top">
                        <p><a href="https://arxiv.org/abs/2209.13797">
                            <papertitle>PCB-RandNet: Rethinking Random Sampling for LIDAR Semantic Segmentation in Autonomous Driving Scene
                            </papertitle>
                        </a>
                            <br><strong>Huixian Cheng</strong>, Xianfeng Han, Hang Jiang, Dehong He, Guoqiang Xiao<br>
                            <em>Submitted to RA-L, 2022</em><br>
                            <a href="https://arxiv.org/abs/2209.13797">Paper</a> /
                            <a href="https://github.com/huixiancheng/PCB-RandNet"><font color="red">Code</font></a>
                            <iframe src="https://ghbtns.com/github-btn.html?user=huixiancheng&repo=PCB-RandNet&type=star&count=true&size=small"
                                    frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
                        <p align="justify" style="font-size:13px"> Fast and efficient semantic segmentation of large-scale LiDAR point clouds is a fundamental problem in autonomous driving. To achieve this goal, the existing point-based methods mainly choose to adopt Random Sampling strategy to process large-scale point clouds. However, our quantative and qualitative studies have found that Random Sampling may be less suitable for the autonomous driving scenario, since the LiDAR points follow an uneven or even long-tailed distribution across the space, which prevents the model from capturing sufficient information from points in different distance ranges and reduces the model's learning capability. To alleviate this problem, we propose a new Polar Cylinder Balanced Random Sampling method that enables the downsampled point clouds to maintain a more balanced distribution and improve the segmentation performance under different spatial distributions. In addition, a sampling consistency loss is introduced to further improve the segmentation performance and reduce the model's variance under different sampling methods. Extensive experiments confirm that our approach produces excellent performance on both SemanticKITTI and SemanticPOSS benchmarks.</p>
                        <p></p>
                    </td>
                </tr>
                </tbody>
            </table>

            <!--SECTION-->
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                <tr>
                    <td width="20%"><img src="images/first.png" alt="PontTuset" width="180"
                                         style="border-style: none"></td>
                    <td width="80%" valign="top">
                        <p><a href="https://arxiv.org/abs/2207.12691">
                            <papertitle>CENet: Toward Concise and Efficient LiDAR Semantic Segmentation for Autonomous Driving
                            </papertitle>
                        </a>
                            <br><strong>Huixian Cheng</strong>, Xianfeng Han, Guoqiang Xiao<br>
                            <em> ICME, 2022</em>
                            <br>
                            <!--<font color="red"><strong>..</strong></font><br>-->
                            <a href="https://arxiv.org/abs/2207.12691">Paper</a> /
                            <a href="https://github.com/huixiancheng/CENet"><font color="red">Code</font></a>
                            <iframe src="https://ghbtns.com/github-btn.html?user=huixiancheng&repo=CENet&type=star&count=true&size=small"
                                    frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
                        <p align="justify" style="font-size:13px">  In this paper, we present a concise and efficient image-based semantic segmentation network, named CENet. In order to improve the descriptive power of learned features and reduce the computational as well as time complexity, our CENet integrates the convolution with larger kernel size instead of MLP, carefully-selected activation functions, and multiple auxiliary segmentation heads with corresponding loss functions into architecture. Quantitative and qualitative experiments conducted on publicly available benchmarks, SemanticKITTI and SemanticPOSS, demonstrate that our pipeline achieves much better mIoU and inference performance compared with state-of-the-art models.</p>
                        <p></p>
                    </td>
                </tr>
                </tbody>
            </table>

            <!--SECTION-->
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                <tr>
                    <td width="20%"><img src="images/trans.png" alt="PontTuset" width="180"
                                         style="border-style: none"></td>
                    <td width="80%" valign="top">
                        <p><a href="paper/TransRVNet.pdf">
                            <papertitle>TransRVNet: LiDAR Semantic Segmentation with Transformer
                            </papertitle>
                        </a>
                            <br><strong>Huixian Cheng</strong>, Xianfeng Han, Guoqiang Xiao<br>
                            <em>Submitted to T-ITS, 2021</em>
                            <br>
                            <a href="paper/TransRVNet.pdf">Paper</a> /
                            <a href="https://github.com/huixiancheng/TransRVNet"><font color="red">Code</font></a>
                            <iframe src="https://ghbtns.com/github-btn.html?user=huixiancheng&repo=TransRVNet&type=star&count=true&size=small"
                                    frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
                        <p align="justify" style="font-size:13px"> In this paper, we present Transformer-Range-View Network (TransRVNet), a novel and powerful projection-based CNN-Transformer architecture to infer point-wise semantics. First, a Multi Residual Channel Interaction Attention Module (MRCIAM) is introduced to capture channel-level multi-scale feature and model intra-channel, inter-channel correlations based on attention mechanism. Then, in the encoder stage, we use a well-designed Residual Context Aggregation Module (RCAM), including a residual dilated convolution structure and a context aggregation module, to fuse information from different receptive fields while reducing the impact of missing points. Finally, a Balanced Non-square-Transformer Module (BNTM) is employed as fundamental component of decoder to achieve locally feature dependencies for more discriminative feature learning by introducing the non-square shifted window strategy. Extensive qualitative and quantitative experiments conducted on challenging SemanticKITTI and SemanticPOSS benchmarks have verified the effectiveness of our proposed technique.</p>
                        <p></p>
                    </td>
                </tr>
                </tbody>
            </table>


            <!--SECTION-->
            <table width="100%" align="center" border="0" cellspacing="0"
                   cellpadding="20">
                <tbody>
                <tr>
                    <td style="padding:0px">
                        <br>
                        <p style="text-align:right;font-size:small;color:gray">
                            Website adapted from this <a href="https://github.com/jonbarron/jonbarron_website">source
                            code</a>.
                        </p>
                    </td>
                </tr>
                </tbody>
            </table>
    </tr>
</table>
</body>
</html>
